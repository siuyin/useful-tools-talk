# Local-first computing with Go
Tools I've used with Go to build software that runs locally.
21 Feb 2025

Loh Siu Yin
Technology Consultant, Beyond Broadcast LLP
siuyin@beyondbroadcast.com

## Tool categories
Programming Language: `Go` https://go.dev/

Serialization and persistence: eg. JSON, boltDB

Communications / transport: eg http, NATS

## Programming
`go` Hello World

.play -edit -numbers cmd/hello/main.go

`fmt` is the `go` standard library formatted output package.

Identifiers starting with an Uppercase character are public or "exported" functions.

## Adding Serialization and Deserialization
.code -numbers cmd/gob/main.go /^type Person/,/^}/
.play -edit -numbers cmd/gob/main.go /^func main/,/^}/

Let's try reading only.

## Writing GOBs
.code -numbers cmd/gob/main.go /^func writeGOB/,/^}/
## Reading GOBs
.code -numbers cmd/gob/main.go /^func readGOB/,/^}/


## Key Value store
.code -numbers cmd/kv/main.go /^func main/,/^}/
.code -numbers cmd/kv/main.go /^func putData/,/^}/

demo: go run cmd/kv/main.go

## Embedded NATS Server
- Core NATS: fire and forget 
  - Publish / Subscribe
  - Request / Reply
  - Queue Groups (load balancing)

- JetStream: at least once or exactly once delivery (within time window)
  - Streams
  - Key Value Stores
  - Object Stores

## ollama : local-first AI Models
ollama https://github.com/ollama/ollama enables running Large Language Models locally.

.code -numbers cmd/llm/main.go /func main/,/^}/

## respFunc and getQuery functions
.code -numbers cmd/llm/main.go /func respFunc/,/^}/
.code -numbers cmd/llm/main.go /func getQuery/,/^}/
.code -numbers cmd/llm/main.go /^func init/,/^}/

## demo: running an LLM locally
Demo dependencies:
- ollama : install ollama : https://ollama.com/download
- Large Language Model: `ollama pull gemma2:2b` 

Also try out `deepseek-r1:1.5b`

demo: go run cmd/llm/*.go

## chromem-go : Embedded Vector Database
Vector search powers Retrieval Augmented Generation (RAG).

RAG provides relevant factual context to Large Language Models to minimise hallucination
when they generate content.

Demo dependencies:
- ollama : install ollama : https://ollama.com/download
- embedding model: `ollama pull nomic-embed-text`

local LLM with gemma2 (2 billion parameter model):
- `ollama pull gemma2:2b`

demo: go run cmd/rag/*.go

## Creating embeddings
.code -numbers cmd/rag/main.go /const embeddingModel/,/$/
.code -numbers cmd/rag/main.go /func loadOrCreateVecDB/,/^}/
